---
phase: 04-llm-content-curation
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - backend/src/backend/prompts.py
  - backend/src/backend/scoring.py
  - backend/src/backend/scoring_queue.py
  - backend/src/backend/scheduler.py
  - backend/src/backend/feeds.py
  - backend/src/backend/main.py
autonomous: true

must_haves:
  truths:
    - "New articles are automatically enqueued for scoring after feed refresh"
    - "Scoring queue processes articles: categorize (step 1), then score non-blocked articles (step 2)"
    - "Blocked categories cause articles to skip interest scoring and auto-score 0"
    - "Composite score is computed as interest_score * topic_weight_multiplier * quality_penalty"
    - "Scoring queue runs on a 30-second interval via APScheduler"
    - "LLM returns structured JSON validated by Pydantic schemas"
    - "Failed scoring attempts set scoring_state to 'failed' without crashing the queue"
  artifacts:
    - path: "backend/src/backend/prompts.py"
      provides: "Pydantic response schemas and prompt templates"
      contains: "class CategoryResponse"
    - path: "backend/src/backend/scoring.py"
      provides: "LLM categorization, scoring, and composite score functions"
      contains: "async def categorize_article"
    - path: "backend/src/backend/scoring_queue.py"
      provides: "Queue manager that processes articles through two-step pipeline"
      contains: "class ScoringQueue"
    - path: "backend/src/backend/scheduler.py"
      provides: "Scoring queue job added to scheduler"
      contains: "process_scoring"
  key_links:
    - from: "backend/src/backend/scoring_queue.py"
      to: "backend/src/backend/scoring.py"
      via: "calls categorize_article and score_article"
      pattern: "from backend.scoring import"
    - from: "backend/src/backend/scoring.py"
      to: "backend/src/backend/prompts.py"
      via: "uses Pydantic schemas for structured output"
      pattern: "from backend.prompts import"
    - from: "backend/src/backend/scheduler.py"
      to: "backend/src/backend/scoring_queue.py"
      via: "scheduled job calls queue.process_next_batch"
      pattern: "from backend.scoring_queue import"
    - from: "backend/src/backend/feeds.py"
      to: "backend/src/backend/scoring_queue.py"
      via: "enqueue new articles after save"
      pattern: "scoring_queue"
---

<objective>
Build the two-step LLM scoring pipeline: Pydantic schemas for structured LLM output, categorization and interest scoring functions using Ollama, a queue manager that processes articles in batches, scheduler integration for background processing, and feed refresh hook to auto-enqueue new articles.

Purpose: This is the core intelligence of the app. Articles get categorized and scored without blocking the UI, following the locked two-step pipeline design.
Output: Working background scoring system that categorizes and scores articles via Ollama.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-llm-content-curation/04-RESEARCH.md
@.planning/phases/04-llm-content-curation/04-CONTEXT.md
@.planning/phases/04-llm-content-curation/04-01-SUMMARY.md
@backend/src/backend/models.py
@backend/src/backend/scheduler.py
@backend/src/backend/feeds.py
@backend/src/backend/config.py
@backend/src/backend/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM prompt schemas and scoring functions</name>
  <files>
    backend/src/backend/prompts.py
    backend/src/backend/scoring.py
  </files>
  <action>
    **prompts.py** — Create Pydantic v2 response schemas and prompt template functions:

    `CategoryResponse(BaseModel)`:
    - `categories: list[str] = Field(description="1-6 topic categories", max_length=10)`
    - `suggested_new: list[str] = Field(default=[], description="New categories not in existing list", max_length=3)`

    `ScoringResponse(BaseModel)`:
    - `interest_score: int = Field(ge=0, le=10, description="Interest match 0-10")`
    - `quality_score: int = Field(ge=0, le=10, description="Content quality 0-10")`
    - `reasoning: str = Field(description="1-2 sentence explanation")`

    Create prompt builder functions:
    - `build_categorization_prompt(article_title: str, article_text: str, existing_categories: list[str]) -> str` — Instructs LLM to assign 1-6 categories from the existing list, suggesting new ones sparingly. Truncate article_text to first 2000 chars. Include the existing categories list with instruction to reuse and normalize to lowercase.
    - `build_scoring_prompt(article_title: str, article_text: str, interests: str, anti_interests: str) -> str` — Instructs LLM to score interest (0-10) and quality (0-10) against user preferences, with 1-2 sentence reasoning. Truncate article_text to first 3000 chars.

    Ship with a `DEFAULT_CATEGORIES` list of ~25 broad seed categories:
    Technology, Science, Politics, Business, Finance, Health, Sports, Entertainment, Culture, Gaming, Programming, AI/ML, Cybersecurity, Climate, Space, Education, Food, Travel, Design, Music, Film, Philosophy, History, Law, Startups

    **scoring.py** — Create the scoring functions using ollama-python:

    First, install ollama: `uv add ollama tenacity`

    `async def categorize_article(article_title: str, article_text: str, existing_categories: list[str], settings) -> CategoryResponse` — Call Ollama AsyncClient with categorization model (from settings.ollama.categorization_model), use `format=CategoryResponse.model_json_schema()` for structured output, temperature=0. Parse response with `CategoryResponse.model_validate_json()`. Wrap with tenacity retry (3 attempts, exponential backoff starting at 2s). Use `AsyncClient(host=settings.ollama.host, timeout=settings.ollama.timeout)`.

    `async def score_article(article_title: str, article_text: str, interests: str, anti_interests: str, settings) -> ScoringResponse` — Same pattern but with scoring model and scoring prompt. Also with tenacity retry.

    `def compute_composite_score(interest_score: int, quality_score: int, categories: list[str], topic_weights: dict[str, str] | None) -> float` — Pure function. Weight mapping: blocked=0.0, low=0.5, neutral=1.0, medium=1.5, high=2.0. Average the topic weights for the article's categories (default to neutral if category not in weights). Quality multiplier: 0.5 + (quality_score / 10.0) * 0.5 (maps 0-10 to 0.5-1.0). Return: interest_score * category_multiplier * quality_multiplier. Cap at 20.0 max.

    `def is_blocked(categories: list[str], topic_weights: dict[str, str] | None) -> bool` — Returns True if ANY category in the list has weight "blocked".

    `async def get_active_categories(session: Session) -> list[str]` — Query articles scored in last 30 days, collect unique categories. Also include non-blocked categories from UserPreferences.topic_weights. Merge with DEFAULT_CATEGORIES. Return sorted, deduplicated (case-insensitive, preserving first occurrence).
  </action>
  <verify>
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.prompts import CategoryResponse, ScoringResponse, DEFAULT_CATEGORIES; print(len(DEFAULT_CATEGORIES), 'seed categories')"`.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.scoring import compute_composite_score; print(compute_composite_score(8, 7, ['technology'], {'technology': 'high'}))"` should print a float around 13.6.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run ruff check src/`.
  </verify>
  <done>
    prompts.py has CategoryResponse, ScoringResponse, prompt builders, and DEFAULT_CATEGORIES. scoring.py has categorize_article, score_article (both async with retry), compute_composite_score (pure function), is_blocked, and get_active_categories. All import cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create scoring queue and wire into scheduler + feed refresh</name>
  <files>
    backend/src/backend/scoring_queue.py
    backend/src/backend/scheduler.py
    backend/src/backend/feeds.py
    backend/src/backend/main.py
  </files>
  <action>
    **scoring_queue.py** — Create `ScoringQueue` class:

    `async def enqueue_articles(self, session: Session, article_ids: list[int])` — Set articles with given IDs to scoring_state="queued" (only if currently "unscored" or "failed"). Commit.

    `async def enqueue_recent_for_rescoring(self, session: Session, days: int = 7, max_articles: int = 100)` — Find unread articles from last `days` days (or max `max_articles`, whichever is smaller). Set their scoring_state to "queued". This powers the "re-score on preference save" feature.

    `async def process_next_batch(self, session: Session, batch_size: int = 5)` — Core processing loop:
    1. Query `batch_size` articles where scoring_state="queued", ordered by published_at ASC (oldest first)
    2. For each article:
       a. Set scoring_state="scoring", commit
       b. Load user preferences (cache for batch)
       c. Get active categories list
       d. Call `categorize_article()` — store result in article.categories (normalize to lowercase)
       e. Check `is_blocked()` — if blocked, set interest_score=0, quality_score=0, composite_score=0.0, reasoning="Blocked: {categories}", scoring_state="scored"
       f. If not blocked, call `score_article()` — store interest_score, quality_score, reasoning
       g. Compute composite_score via `compute_composite_score()`
       h. Set scoring_state="scored", scored_at=datetime.now()
       i. Commit after each article (not batch — partial progress survives failures)
    3. On exception for individual article: set scoring_state="failed", log error, continue to next article

    Import settings via `get_settings()` and pass to scoring functions.

    **scheduler.py** — Extend `start_scheduler()`:
    - Import ScoringQueue
    - Create module-level `scoring_queue = ScoringQueue()` instance
    - Add `async def process_scoring_queue()` function that gets a session and calls `scoring_queue.process_next_batch(session, batch_size=5)`
    - Register as interval job: every 30 seconds, id="process_scoring", replace_existing=True
    - Export `scoring_queue` so other modules can call `enqueue_articles`

    **feeds.py** — Hook scoring into feed refresh:
    - In `save_articles()`, after committing new articles, collect the IDs of newly created articles
    - After the commit, import and call `scoring_queue.enqueue_articles(session, new_article_ids)` to queue them for scoring
    - This requires `save_articles` to return article IDs or enqueue inline. Simplest approach: after the commit loop, query for articles with the URLs we just saved to get their IDs, then enqueue. Or: collect IDs from `session.refresh(article)` after add. Choose the cleaner approach.

    **main.py** — Add a `POST /api/scoring/rescore` endpoint:
    - Loads preferences, calls `scoring_queue.enqueue_recent_for_rescoring(session)`, returns `{"queued": count}`
    - This is called by the settings page after saving preferences (wired in Plan 03)
    - Also update the PUT /api/preferences endpoint to trigger re-scoring inline (call enqueue_recent_for_rescoring after saving preferences), so the frontend just needs one call to save+rescore

    Also add `GET /api/scoring/status` endpoint:
    - Returns counts by scoring_state: `{"unscored": N, "queued": N, "scoring": N, "scored": N, "failed": N}`
    - Simple query using group_by on scoring_state
  </action>
  <verify>
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run ruff check src/`.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run pytest tests/ -x` to ensure existing tests pass.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.scoring_queue import ScoringQueue; print('Queue OK')"`.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.scheduler import scoring_queue; print('Scheduler wired OK')"`.
  </verify>
  <done>
    ScoringQueue processes articles through the two-step pipeline (categorize then score). Blocked categories auto-score 0 and skip step 2. Scheduler runs queue every 30 seconds. New articles from feed refresh are auto-enqueued. PUT /api/preferences triggers re-scoring of recent articles. GET /api/scoring/status returns queue state counts.
  </done>
</task>

</tasks>

<verification>
- `uv run ruff check src/` passes
- `uv run pytest tests/ -x` passes
- ScoringQueue, scoring functions, and prompts all import without error
- Scheduler registers both feed_refresh and process_scoring jobs
- feeds.py enqueues new articles after saving
- compute_composite_score returns correct values for known inputs
</verification>

<success_criteria>
Two-step LLM pipeline operational: categorization assigns tags, scoring evaluates interest, composite score computed. Queue processes articles in background via scheduler. New articles auto-enqueue on feed refresh. Preference save triggers re-scoring. Blocked categories skip scoring. Failed articles don't crash the queue.
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-content-curation/04-02-SUMMARY.md`
</output>
