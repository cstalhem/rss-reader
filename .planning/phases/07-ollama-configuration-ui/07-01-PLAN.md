---
phase: 07-ollama-configuration-ui
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/backend/ollama_service.py
  - backend/src/backend/models.py
  - backend/src/backend/database.py
  - backend/src/backend/main.py
  - backend/src/backend/scoring.py
  - backend/src/backend/scoring_queue.py
autonomous: true

must_haves:
  truths:
    - "GET /api/ollama/health returns connected status, version, and latency when Ollama is reachable"
    - "GET /api/ollama/health returns connected=false when Ollama is unreachable"
    - "GET /api/ollama/models returns list of locally available models with name, size, and is_loaded status"
    - "POST /api/ollama/models/pull streams SSE progress events during model download"
    - "POST /api/ollama/models/pull/cancel aborts an active download"
    - "DELETE /api/ollama/models/{name} removes a model from Ollama"
    - "GET /api/ollama/download-status returns current download state for navigate-away resilience"
    - "GET /api/ollama/config returns runtime model config from UserPreferences (with YAML fallback)"
    - "PUT /api/ollama/config saves model choices to UserPreferences and optionally enqueues re-scoring"
    - "GET /api/ollama/prompts returns current system prompt texts"
    - "Scoring pipeline reads model names from UserPreferences per-batch instead of module-level settings"
    - "Re-scored articles get priority in the scoring queue"
    - "Score-only re-scoring skips categorization when only scoring model changed"
  artifacts:
    - path: "backend/src/backend/ollama_service.py"
      provides: "Ollama API wrapper (health, models, pull, delete, download state)"
    - path: "backend/src/backend/models.py"
      provides: "UserPreferences extended with ollama_categorization_model, ollama_scoring_model, ollama_use_separate_models"
    - path: "backend/src/backend/database.py"
      provides: "Migration for new UserPreferences columns and Article scoring_priority/rescore_mode columns"
    - path: "backend/src/backend/main.py"
      provides: "All /api/ollama/* endpoints"
    - path: "backend/src/backend/scoring_queue.py"
      provides: "Two-tier config reads, priority ordering, score-only mode"
    - path: "backend/src/backend/scoring.py"
      provides: "Accept runtime model config instead of settings object for model names"
  key_links:
    - from: "backend/src/backend/main.py"
      to: "backend/src/backend/ollama_service.py"
      via: "endpoint handlers calling service functions"
      pattern: "ollama_service\\."
    - from: "backend/src/backend/scoring_queue.py"
      to: "backend/src/backend/models.py"
      via: "per-batch DB read of UserPreferences for model names"
      pattern: "UserPreferences"
    - from: "backend/src/backend/main.py"
      to: "backend/src/backend/scoring_queue.py"
      via: "PUT /api/ollama/config triggers re-scoring enqueue"
      pattern: "scoring_queue\\.enqueue"
---

<objective>
Build the complete backend for Ollama runtime configuration: service layer, API endpoints, DB migration, and two-tier config integration with the scoring pipeline.

Purpose: This is the foundation for the entire phase. All frontend plans depend on these APIs. The two-tier config pattern (Pydantic Settings for infrastructure, UserPreferences for runtime model choices) is the key architectural change.

Output: 6 modified/created backend files providing all /api/ollama/* endpoints and a scoring pipeline that reads model config from the database per-batch.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-ollama-configuration-ui/07-RESEARCH.md
@.planning/phases/07-ollama-configuration-ui/07-CONTEXT.md
@backend/src/backend/config.py
@backend/src/backend/models.py
@backend/src/backend/database.py
@backend/src/backend/main.py
@backend/src/backend/scoring.py
@backend/src/backend/scoring_queue.py
@backend/src/backend/prompts.py
@backend/src/backend/scheduler.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Ollama service layer, DB migration, and model extension</name>
  <files>
    backend/src/backend/ollama_service.py
    backend/src/backend/models.py
    backend/src/backend/database.py
  </files>
  <action>
    **Create `backend/src/backend/ollama_service.py`** -- a thin wrapper around the Ollama Python AsyncClient and httpx for health checks. All functions accept `host: str` as a parameter (read from Pydantic Settings by callers, not imported directly).

    Functions to implement:
    - `check_health(host: str) -> dict` -- httpx GET to `{host}/api/version` with 5s timeout. Returns `{connected: bool, version: str|None, latency_ms: int|None}`. Catches all exceptions and returns connected=False.
    - `list_models(host: str) -> list[dict]` -- Uses `AsyncClient(host=host).list()` combined with `AsyncClient(host=host).ps()` to return enriched model list. Each model: `{name: str, size: int, parameter_size: str|None, quantization_level: str|None, is_loaded: bool}`. The `is_loaded` flag is True when the model name appears in `ps()` results.
    - `pull_model_stream(host: str, model: str)` -- Async generator that yields dicts from `AsyncClient.pull(model, stream=True)`. Each yield: `{status, completed, total, digest}`. Updates module-level `_download_state` dict as chunks arrive. Clears state in finally block.
    - `cancel_download()` -- Sets a module-level `_cancel_requested` flag. The `pull_model_stream` generator checks this flag each iteration and raises `asyncio.CancelledError` if set.
    - `get_download_status() -> dict` -- Returns copy of `_download_state` dict: `{active: bool, model: str|None, completed: int, total: int, status: str|None}`.
    - `delete_model(host: str, model: str) -> dict` -- Calls `AsyncClient.delete(model)`. Returns `{status: "success"}` or raises.

    Module-level state (safe in single-worker asyncio):
    ```python
    _download_state: dict = {"active": False, "model": None, "completed": 0, "total": 0, "status": None}
    _cancel_requested: bool = False
    ```

    **Extend `backend/src/backend/models.py`** -- Add three new optional fields to `UserPreferences`:
    - `ollama_categorization_model: str | None = Field(default=None)` -- runtime override for categorization model
    - `ollama_scoring_model: str | None = Field(default=None)` -- runtime override for scoring model
    - `ollama_use_separate_models: bool = Field(default=False)` -- whether to use separate models

    Add two new fields to `Article`:
    - `scoring_priority: int = Field(default=0)` -- 1 for re-score priority, 0 for normal
    - `rescore_mode: str | None = Field(default=None)` -- "full" or "score_only" or None for normal

    **Extend `backend/src/backend/database.py`** -- Add a migration function `_migrate_ollama_config_columns()` that:
    1. Checks `user_preferences` table for missing columns: `ollama_categorization_model (TEXT)`, `ollama_scoring_model (TEXT)`, `ollama_use_separate_models (BOOLEAN DEFAULT 0)`
    2. Checks `articles` table for missing columns: `scoring_priority (INTEGER DEFAULT 0)`, `rescore_mode (TEXT)`
    3. Adds any missing columns via ALTER TABLE
    4. Call this function from `create_db_and_tables()` after existing migrations

    Follow the exact pattern of `_migrate_articles_scoring_columns()` for the migration style.
  </action>
  <verify>
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.ollama_service import check_health, list_models, get_download_status; print('Service OK')"` to confirm imports work.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.models import UserPreferences, Article; print(UserPreferences.model_fields.keys()); print('scoring_priority' in Article.model_fields)"` to confirm new fields exist.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run ruff check src/` to confirm no lint errors.
  </verify>
  <done>
    ollama_service.py exists with all 6 functions. UserPreferences has 3 new fields. Article has 2 new fields. database.py migration handles all 5 new columns. All imports resolve and lint passes.
  </done>
</task>

<task type="auto">
  <name>Task 2: API endpoints and two-tier scoring pipeline</name>
  <files>
    backend/src/backend/main.py
    backend/src/backend/scoring.py
    backend/src/backend/scoring_queue.py
  </files>
  <action>
    **Add API endpoints to `backend/src/backend/main.py`:**

    Add these Pydantic request/response models near the top:
    - `OllamaHealthResponse(connected: bool, version: str|None, latency_ms: int|None)`
    - `OllamaModelResponse(name: str, size: int, parameter_size: str|None, quantization_level: str|None, is_loaded: bool)`
    - `PullModelRequest(model: str)`
    - `OllamaConfigResponse(categorization_model: str, scoring_model: str, use_separate_models: bool)`
    - `OllamaConfigUpdate(categorization_model: str, scoring_model: str, use_separate_models: bool, rescore: bool = False)`
    - `OllamaPromptsResponse(categorization_prompt: str, scoring_prompt: str)`

    Add endpoints:
    1. `GET /api/ollama/health` -- Calls `ollama_service.check_health(settings.ollama.host)`. Returns `OllamaHealthResponse`.
    2. `GET /api/ollama/models` -- Calls `ollama_service.list_models(settings.ollama.host)`. Returns `list[OllamaModelResponse]`.
    3. `POST /api/ollama/models/pull` -- Accepts `PullModelRequest`. Returns `StreamingResponse` with `media_type="text/event-stream"`. Iterates over `ollama_service.pull_model_stream()`, yielding `data: {json}\n\n` per chunk.
    4. `POST /api/ollama/models/pull/cancel` -- Calls `ollama_service.cancel_download()`. Returns `{status: "cancelled"}`.
    5. `DELETE /api/ollama/models/{name:path}` -- Calls `ollama_service.delete_model()`. Returns `{status: "success"}`. Use `:path` type to handle model names with colons (e.g., `qwen3:8b`).
    6. `GET /api/ollama/download-status` -- Calls `ollama_service.get_download_status()`. Returns the dict.
    7. `GET /api/ollama/config` -- Reads UserPreferences from DB. Returns runtime model config. Falls back to `settings.ollama.categorization_model` / `settings.ollama.scoring_model` when DB values are None.
    8. `PUT /api/ollama/config` -- Accepts `OllamaConfigUpdate`. Saves to UserPreferences (reassign fields, don't mutate). If `rescore=True`, determine `rescore_mode` by comparing old vs new config:
       - If categorization model changed: mode = "full"
       - Else if scoring model changed: mode = "score_only"
       - Else: no re-score needed
       Then enqueue all unread scored articles for re-scoring with `scoring_priority=1` and the determined `rescore_mode`. Return the saved config plus `{rescore_queued: int}`.
    9. `GET /api/ollama/prompts` -- Import `build_categorization_prompt` and `build_scoring_prompt` from prompts.py. Call them with placeholder values (e.g., "[Article Title]", "[Article Content]", etc.) to get the prompt templates. Return both as strings.

    **Modify `backend/src/backend/scoring.py`:**

    Change `categorize_article()` and `score_article()` to read model names from an explicit parameter rather than `settings.ollama.categorization_model`/`settings.ollama.scoring_model`. The `settings` parameter is still passed but now the functions should use `settings.ollama.host` for the host and `settings.ollama.thinking` for the thinking flag. Add a new parameter `model: str` to both functions. The caller (scoring_queue) will resolve which model to use.

    So the signatures become:
    - `categorize_article(article_title, article_text, existing_categories, settings, model: str)`
    - `score_article(article_title, article_text, interests, anti_interests, settings, model: str)`

    Replace `settings.ollama.categorization_model` with `model` and `settings.ollama.scoring_model` with `model` in the respective functions.

    **Modify `backend/src/backend/scoring_queue.py`:**

    1. Change `process_next_batch` to read runtime model config from the database at the start of each batch (not from the module-level `settings`):
       - Read `UserPreferences` from the session (already done for preferences)
       - Determine categorization model: `preferences.ollama_categorization_model or settings.ollama.categorization_model`
       - Determine scoring model: if `preferences.ollama_use_separate_models` is True, use `preferences.ollama_scoring_model or settings.ollama.scoring_model`; otherwise use the same model as categorization
       - Pass the resolved model name to `categorize_article()` and `score_article()` via the new `model` parameter

    2. Change the queue query to order by priority: `ORDER BY scoring_priority DESC, published_at ASC` so re-scored articles are processed first.

    3. Handle `rescore_mode` on articles:
       - If `article.rescore_mode == "score_only"`: skip `categorize_article()`, keep existing categories, go directly to `score_article()`
       - If `article.rescore_mode == "full"` or None: run full pipeline as before
       - After scoring, clear `rescore_mode` and `scoring_priority` back to defaults (None and 0)

    4. Keep the module-level `settings = get_settings()` for infrastructure values (host, thinking, scheduler intervals) -- only model names come from DB now.
  </action>
  <verify>
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run ruff check src/` to confirm no lint errors.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run ruff format --check src/` to confirm formatting.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.main import app; routes = [r.path for r in app.routes]; assert '/api/ollama/health' in routes; assert '/api/ollama/models' in routes; assert '/api/ollama/config' in routes; print('All routes registered')"` to verify endpoint registration.
    Run `cd /Users/cstalhem/projects/rss-reader/backend && uv run pytest` to check existing tests still pass.
  </verify>
  <done>
    All 9 /api/ollama/* endpoints are registered and importable. scoring.py functions accept explicit model parameter. scoring_queue.py reads model config from DB per-batch, orders by priority, and handles rescore_mode. Existing tests pass. Lint clean.
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/cstalhem/projects/rss-reader/backend && uv run ruff check src/ && uv run ruff format --check src/` -- lint and format clean
2. `cd /Users/cstalhem/projects/rss-reader/backend && uv run pytest` -- existing tests pass
3. `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.main import app; print([r.path for r in app.routes if 'ollama' in r.path])"` -- all ollama routes present
4. `cd /Users/cstalhem/projects/rss-reader/backend && uv run python -c "from backend.models import UserPreferences; assert 'ollama_categorization_model' in UserPreferences.model_fields"` -- model fields exist
</verification>

<success_criteria>
- All 9 /api/ollama/* endpoints exist and are syntactically correct
- ollama_service.py encapsulates all Ollama API interactions
- UserPreferences has 3 new fields (ollama_categorization_model, ollama_scoring_model, ollama_use_separate_models)
- Article has 2 new fields (scoring_priority, rescore_mode)
- database.py migrates all 5 new columns
- scoring_queue.py reads model config from DB per-batch and orders by priority
- scoring.py accepts explicit model parameter
- SSE streaming endpoint for pull progress returns properly formatted events
- No import errors, lint clean, existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-ollama-configuration-ui/07-01-SUMMARY.md`
</output>
