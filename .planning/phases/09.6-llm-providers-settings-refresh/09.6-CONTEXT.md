# Phase 09.6: UI refresh of LLM Providers settings section - Context

**Gathered:** 2026-02-24
**Status:** Ready for planning

<domain>
## Phase Boundary

Redesign the LLM Providers settings page to separate provider configuration from model management, introduce a multi-provider UI with per-provider settings surfaces, and prepare the architecture for additional providers (OpenAI, Anthropic, Google, OpenRouter) without implementing their backends.

Only Ollama is functional in this phase. The other 4 providers appear in the "Add Provider" dialog with "Coming soon" badges but cannot be added or configured.

**Prerequisite:** Branch must be rebased onto latest dev before research/planning (done: picks up editable host/port and provider-as-plugins foundation from `555006c` and later commits).

</domain>

<decisions>
## Implementation Decisions

### Provider picker layout
- Configured providers show as **pills** (logo + name) in a horizontal bar
- An **"Add Provider" button** next to the pills opens a dialog with provider cards
- Add dialog cards show: provider logo, name, 1-line description, and setup hint ("Local -- no API key" for Ollama, "Requires API key" for cloud providers)
- Unimplemented providers (OpenAI, Anthropic, Google, OpenRouter) have a **"Coming soon" badge overlay** on the same card design -- not clickable/selectable
- **Empty state** (no providers configured): EmptyState component with "No providers configured" message and prominent "Add Provider" button
- **Clean slate approach**: Ollama is NOT pre-configured -- users go through the Add Provider flow for all providers, including Ollama
- Click a pill to **toggle its settings panel** below the pill bar (only one open at a time)
- **Disconnect button** lives inside the provider's expanded settings panel (not on the pill itself)

### Page structure (top to bottom)
1. **Model selector** (top-level): grouped-by-provider searchable dropdown showing "exposed" models from all active providers
   - Separate-models toggle stays (categorization vs scoring model)
   - **Mix-and-match allowed**: user can select models from different providers (e.g., Ollama for categorization, OpenAI for scoring)
   - Save & Rescore button stays together at this level
2. **Provider pills** + expandable per-provider settings panels (middle)
3. **System prompts** (bottom): collapsed, accessible, provider-agnostic section -- same prompts apply to any provider's LLM calls

### Per-provider settings (Ollama)
- **Connection info**: editable host/port (already runtime-configurable on dev branch)
- **Health status**: colored provider icon on the pill with tooltip showing connection state (connected/disconnected) -- only Ollama needs this since it depends on a local server
- **Model library**: download/delete/manage installed Ollama models -- moves from top-level into Ollama's provider panel. Light visual refresh to align with new card/pill aesthetic
- Each provider determines which models are "exposed" to the top-level selector:
  - For Ollama: exposed = downloaded/installed models (current behavior)
  - For API providers (future): user would select which API models to make available from the full catalog

### Provider status & discovery
- Provider pills show **brand icon** with optional status indicator (Ollama only: colored icon for health)
- Settings sidebar stays **plain** -- no status indicators on the "LLM Providers" nav item
- **Actual brand SVG logos** for all 5 providers (Ollama, OpenAI, Anthropic, Google, OpenRouter) -- to be sourced collaboratively during implementation

### Claude's Discretion
- Exact pill bar styling and spacing
- Settings panel expand/collapse animation
- Model library visual refresh details (within "light refresh" scope)
- Empty state illustration or icon choice
- Tooltip implementation details for health status
- Searchable dropdown component choice (Chakra Combobox or custom)

</decisions>

<specifics>
## Specific Ideas

- Provider picker pattern inspired by the Feeds settings page: pills for configured items + "Add" button that opens a dialog
- Top-level model selector needs grouped dropdown with search -- user wants to find models by name across all providers
- Provider-as-plugins foundation already exists in codebase (`555006c` commit) -- research phase should analyze this and align with it
- System prompts stay read-only for now but should be placed accessibly, anticipating a future phase where users can edit them (possibly with a variable/template system)
- The "thinking" toggle (ollama_thinking in DB but never exposed in UI) is explicitly NOT surfaced in this phase

</specifics>

<deferred>
## Deferred Ideas

- Editable system prompts with variable/template system for category lists -- future phase
- Backend implementation for OpenAI, Anthropic, Google, OpenRouter providers -- separate phases per provider
- Surfacing the `ollama_thinking` toggle -- evaluate if/when to expose

</deferred>

---

*Phase: 09.6-llm-providers-settings-refresh*
*Context gathered: 2026-02-24*
